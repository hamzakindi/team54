{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib:matplotlib data path: c:\\RnD\\MedHack\\team54\\data-science-project\\.venv\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=C:\\Users\\humza\\.matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is win32\n",
      "DEBUG:matplotlib:CACHEDIR=C:\\Users\\humza\\.matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from C:\\Users\\humza\\.matplotlib\\fontlist-v390.json\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('C:/RnD/MedHack/team54/data-science-project')\n",
    "sys.path.append(str(project_root))\n",
    "# Add src directory to path\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "from src.data.preprocessing import *\n",
    "from src.models.model import ModelTrainer\n",
    "from src.utils.helpers import plot_data_distribution, calculate_metrics, log_message\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Notebook\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading and preprocessing data\n",
    "2. Model initialization and training\n",
    "3. Performance evaluation\n",
    "4. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: allergies.csv\n",
      "Successfully loaded: conditions.csv\n",
      "Successfully loaded: devices.csv\n",
      "Successfully loaded: encounters.csv\n",
      "Successfully loaded: imaging.csv\n",
      "Successfully loaded: immunizations.csv\n",
      "Successfully loaded: medications.csv\n",
      "Successfully loaded: observations.csv\n",
      "Successfully loaded: patients.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Data loaded successfully. Keys: ['allergies', 'conditions', 'devices', 'encounters', 'imaging', 'immunizations', 'medications', 'observations', 'patients', 'procedures']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: procedures.csv\n",
      "Available datasets:\n",
      "- allergies\n",
      "- conditions\n",
      "- devices\n",
      "- encounters\n",
      "- imaging\n",
      "- immunizations\n",
      "- medications\n",
      "- observations\n",
      "- patients\n",
      "- procedures\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dict = load_data()\n",
    "\n",
    "logging.debug(f\"Data loaded successfully. Keys: {list(data_dict.keys())}\")\n",
    "\n",
    "# Display available datasets\n",
    "print(\"Available datasets:\")\n",
    "for name in data_dict.keys():\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup and Training\n",
    "Choose a dataset and target column for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: allergies.csv\n",
      "Successfully loaded: conditions.csv\n",
      "Successfully loaded: devices.csv\n",
      "Successfully loaded: encounters.csv\n",
      "Successfully loaded: imaging.csv\n",
      "Successfully loaded: immunizations.csv\n",
      "Successfully loaded: medications.csv\n",
      "Successfully loaded: observations.csv\n",
      "Successfully loaded: patients.csv\n",
      "Successfully loaded: procedures.csv\n",
      "Available datasets: ['allergies', 'conditions', 'devices', 'encounters', 'imaging', 'immunizations', 'medications', 'observations', 'patients', 'procedures']\n",
      "Error during model training: 'ModelTrainer' object has no attribute 'inspect_data_size'\n"
     ]
    }
   ],
   "source": [
    "# Reload modules to get latest changes\n",
    "import importlib\n",
    "import src.data.preprocessing\n",
    "import src.models.model\n",
    "import src.utils.helpers\n",
    "\n",
    "importlib.reload(src.data.preprocessing)\n",
    "importlib.reload(src.models.model)\n",
    "importlib.reload(src.utils.helpers)\n",
    "\n",
    "# Import required classes and functions\n",
    "from src.data.preprocessing import load_data\n",
    "from src.models.model import ModelTrainer\n",
    "from src.utils.helpers import plot_data_distribution, calculate_metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load and verify data\n",
    "data_dict = load_data()\n",
    "print(\"Available datasets:\", list(data_dict.keys()))\n",
    "\n",
    "\n",
    "# Initialize model with preprocessing\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Reduce number of trees for initial testing\n",
    "    max_depth=10,      # Limit tree depth\n",
    "    n_jobs=-1         # Use all CPU cores\n",
    ")\n",
    "trainer = ModelTrainer(model)\n",
    "\n",
    "try:\n",
    "    # Sample data for development (remove for production)\n",
    "    sample_size = 10000  # Adjust based on your memory constraints\n",
    "    data_dict['conditions'] = data_dict['conditions'].sample(n=min(sample_size, len(data_dict['conditions'])), random_state=42)\n",
    "    \n",
    "    # Check data size and optimize memory\n",
    "    trainer.inspect_data_size(data_dict['conditions'])  # Check size first\n",
    "    data_dict['conditions'] = trainer.optimize_dtypes(data_dict['conditions'])  # Optimize memory\n",
    "\n",
    "    # Prepare data with automatic handling of categorical features\n",
    "    trainer.prepare_data(data_dict, dataset_name='conditions', target_column='DESCRIPTION')\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate model\n",
    "    score = trainer.evaluate()\n",
    "    print(f\"\\nModel Score: {score:.4f}\")\n",
    "    \n",
    "    # Optional: Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "    trainer.tune_hyperparameters(param_grid)\n",
    "    \n",
    "    # Save the best model\n",
    "    model_path = 'models/random_forest.joblib'\n",
    "    trainer.save_model(model_path)\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during model training: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 17:38:30,531 - src.models.model - ERROR - Error in model evaluation: Data not prepared. Call prepare_data() first\n",
      "ERROR:src.models.model:Error in model evaluation: Data not prepared. Call prepare_data() first\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data not prepared. Call prepare_data() first",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# First check if data is prepared\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trainer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mX_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData not prepared. Call prepare_data() first\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Then check if model exists and is fitted\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trainer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Data not prepared. Call prepare_data() first"
     ]
    }
   ],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Model evaluation and visualization\n",
    "try:\n",
    "    # First check if data is prepared\n",
    "    if not hasattr(trainer, 'X_test') or trainer.X_test is None:\n",
    "        raise ValueError(\"Data not prepared. Call prepare_data() first\")\n",
    "        \n",
    "    # Then check if model exists and is fitted\n",
    "    if not hasattr(trainer, 'model') or trainer.model is None:\n",
    "        raise ValueError(\"Model not initialized\")\n",
    "    \n",
    "    try:\n",
    "        trainer.model.predict(trainer.X_test[:1])\n",
    "    except (NotFittedError, AttributeError):\n",
    "        raise NotFittedError(\"Model must be trained before evaluation. Call train() first\")\n",
    "    \n",
    "    # Get predictions and metrics\n",
    "    y_pred = trainer.model.predict(trainer.X_test)\n",
    "    metrics = calculate_metrics(trainer.y_test, y_pred)\n",
    "    \n",
    "    # Display metrics with formatting\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Model Performance Metrics - {trainer.dataset_name}\")\n",
    "    print(\"=\"*50)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.title():>15}: {value:.4f}\")\n",
    "        trainer.logger.info(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    if hasattr(trainer.model, 'feature_importances_'):\n",
    "        # Create feature importance DataFrame\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': trainer.X_train.columns,\n",
    "            'importance': trainer.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Plot settings\n",
    "        plt.style.use('seaborn')\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Plot top N features\n",
    "        top_n = min(20, len(feature_importance))\n",
    "        sns.barplot(\n",
    "            data=feature_importance.head(top_n),\n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            palette='viridis',\n",
    "            ax=ax\n",
    "        )\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_title(f'Top {top_n} Feature Importance\\nModel: {type(trainer.model).__name__}',\n",
    "                    pad=20, fontsize=12)\n",
    "        ax.set_xlabel('Importance Score', fontsize=10)\n",
    "        ax.set_ylabel('Features', fontsize=10)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(feature_importance.head(top_n)['importance']):\n",
    "            ax.text(v, i, f'{v:.3f}', va='center', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Log top features\n",
    "        trainer.logger.info(\"\\nTop 5 important features:\")\n",
    "        for idx, row in feature_importance.head().iterrows():\n",
    "            trainer.logger.info(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "        # Save feature importance plot\n",
    "        try:\n",
    "            plot_path = Path('plots')\n",
    "            plot_path.mkdir(exist_ok=True)\n",
    "            fig.savefig(plot_path / f'feature_importance_{trainer.dataset_name}.png',\n",
    "                       bbox_inches='tight', dpi=300)\n",
    "            trainer.logger.info(f\"Feature importance plot saved to: {plot_path}\")\n",
    "        except Exception as e:\n",
    "            trainer.logger.warning(f\"Could not save plot: {str(e)}\")\n",
    "    \n",
    "    else:\n",
    "        trainer.logger.warning(\n",
    "            f\"Model {type(trainer.model).__name__} doesn't support feature importance visualization\"\n",
    "        )\n",
    "\n",
    "except NotFittedError as e:\n",
    "    trainer.logger.error(f\"Model not fitted: {str(e)}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    trainer.logger.error(f\"Error in model evaluation: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "trainer.tune_hyperparameters(param_grid, trainer.X_train, trainer.y_train)\n",
    "\n",
    "# Save the best model\n",
    "trainer.save_model('best_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
